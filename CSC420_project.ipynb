{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CSC420_project.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"EQ9xxOBh92jU","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vNKW6nGinJTA","colab_type":"code","colab":{}},"source":["from google.colab import files\n","!cp -r \"/content/drive/My Drive/CSC420P/data\" ."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xv_LRWD9WBNo","colab_type":"code","colab":{}},"source":["!pip install pyts"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BfiQmnt3VFmp","colab_type":"code","colab":{}},"source":["from PIL import Image\n","import glob\n","import numpy as np\n","import torch\n","from torch.utils.data.dataset import Dataset\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import os\n","import cv2\n","from pyts.image import RecurrencePlot, GramianAngularField\n","from mpl_toolkits.axes_grid1 import ImageGrid\n","from PIL import Image\n","\n","class GetData(Dataset):\n","\n","    def __init__(self, image_path, masks):\n","        self.masks = masks\n","        self.images_path = glob.glob(str(image_path) + str(\"/*\"))\n","        self.data_len = len(self.images_path)\n","\n","    def __getitem__(self, index):\n","        single_img = self.images_path[index]\n","        img = Image.open(single_img)  # .convert('L')\n","        img_np = np.array(img,dtype=np.float)\n","        img_np = cv2.resize(img_np,(128,128))\n","        img_np = img_np.transpose((2, 0, 1))\n","        img_list = [1-(image / 255) for image in img_np]\n","        img_tensors = torch.tensor(img_list).float().cuda()\n","\n","        mask_tensors = torch.tensor([int(self.masks[index][1])]).float().cuda()\n","\n","        return (img_tensors, mask_tensors)\n","\n","    def __len__(self):\n","        return self.data_len\n","\n","import pandas as pd\n","\n","\n","def prep_data(img_path, label_path):\n","    labels = pd.read_csv(label_path)\n","    labels = labels.values\n","    dataset = GetData(img_path, labels)\n","    dataset_size = len(dataset)\n","    indices = list(range(dataset_size))\n","    split = int(np.floor(0.15 * dataset_size))\n","    np.random.seed(420)\n","    np.random.shuffle(indices)\n","    train_i, val_i, test_i = indices[:dataset_size-2*split], indices[dataset_size-2*split:dataset_size-split], indices[dataset_size-split:]\n","    train_sampler = torch.utils.data.SubsetRandomSampler(train_i)\n","    valid_sampler = torch.utils.data.SubsetRandomSampler(val_i)\n","    test_sampler = torch.utils.data.SubsetRandomSampler(test_i)\n","    trainLoad = torch.utils.data.DataLoader(dataset=dataset, num_workers=0, batch_size=100, sampler=train_sampler)\n","    validLoad = torch.utils.data.DataLoader(dataset=dataset, num_workers=0, batch_size=75, sampler=valid_sampler)\n","    testLoad = torch.utils.data.DataLoader(dataset=dataset, num_workers=0, batch_size=75, sampler=test_sampler)\n","\n","    return trainLoad, validLoad, testLoad\n","\n","def feature_points(img, start_sigma=0.5, k=1.414):\n","    scales = 5\n","    N,M = img.shape[:2]\n","    img = np.array(img[:,:,0],np.double)\n","    dog = np.zeros((N,M,scales+1), dtype=np.double)\n","    for i in range(scales+1):\n","        dog[:,:,i] = cv2.GaussianBlur(img, (11,11), (k**i)*start_sigma)\n","    for i in range(scales):\n","        dog[:,:,i] = dog[:,:,i+1] - dog[:,:,i]\n","    dog = np.abs(dog)\n","    #m = np.max(dog, axis=0)\n","    #m = np.max(m, axis=0)\n","    dog = dog/np.max(dog[:,:,:scales])\n","    ys, xs, scale = np.where(dog[:,:,:scales] > 0.7)\n","    features = []\n","    #print(scale)\n","    for i in range(ys.shape[0]):\n","        y = ys[i]\n","        x = xs[i]\n","        p = np.argmax(dog[y,x,:scales])\n","        if p == scale[i]:\n","            patch = dog[y-1:y+1,x-1:x+1,scale[i]]\n","            #print(patch)\n","            m = np.max(patch)\n","            if m == dog[y,x,scale[i]]:\n","                features.append((y, x,(k**scale[i])*start_sigma))\n","    return features\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"p_l5p9vbVdJi","colab_type":"code","colab":{}},"source":["\n","\n","# data creation\n","data = pd.read_csv(\"/content/drive/My Drive/data_stocks.csv\")\n","data = data.drop(['DATE'], 1)\n","data = data.drop(['SP500'], 1)\n","\n","r, c = data.values.shape\n","# print(data.columns.values[0])\n","# print(data.values[:, 0][:-30])\n","# print(data.values[:, 0][-32])\n","# print(data.values[:, 0][-31])\n","\n","\n","if not os.path.exists(\"./data/original\"):\n","    os.makedirs(\"./data/original\")\n","if not os.path.exists(\"./data/change1\"):\n","    os.makedirs(\"./data/change1\")\n","if not os.path.exists(\"./data/change2\"):\n","    os.makedirs(\"./data/change2\")\n","if not os.path.exists(\"./data/RCplot\"):\n","    os.makedirs(\"./data/RCplot\")\n","if not os.path.exists(\"./data/RCplot1\"):\n","    os.makedirs(\"./data/RCplot1\")\n","if not os.path.exists(\"./data/RCplot2\"):\n","    os.makedirs(\"./data/RCplot2\")\n","\n","prediction = []\n","for i in range(c): #len(c)\n","    plt.plot(data.values[:, i][:-30])\n","    plt.axis('off')\n","    plt.savefig(\"./data/original/{}.jpg\".format(data.columns.values[i]))\n","    plt.clf()\n","    avg_30 = np.sum(data.values[:, i][-30:])/30\n","    if data.values[:, i][-31] > avg_30:  # why?\n","        prediction.append(0)\n","    else:\n","        prediction.append(1)\n","\n","    rp = RecurrencePlot()\n","    X_rp = rp.fit_transform(np.array([list(range(0,r//4)), data.values[:, i][:r//4]]))    # large or less data?\n","    plt.imshow(X_rp[1], cmap='gist_ncar', origin='lower')\n","    plt.tight_layout()\n","    plt.axis('off')\n","    plt.savefig(\"./data/RCplot/{}.jpg\".format(data.columns.values[i]))\n","    plt.clf()\n","    X_rp = rp.fit_transform(np.array([list(range(3*r//4,r-30)), data.values[:, i][3*r//4:r-30]]))    # large or less data? \n","    plt.imshow(X_rp[1], cmap='gist_ncar', origin='lower')\n","    plt.tight_layout()\n","    plt.axis('off')\n","    plt.savefig(\"./data/RCplot2/{}.jpg\".format(data.columns.values[i]))\n","    plt.clf()\n","    del X_rp, rp\n","\n","labels = np.array([data.columns.values, prediction]) # only 20 change to all\n","labels = labels.T\n","df = pd.DataFrame(labels, columns=[\"Name\", \"Prediction\"])\n","df.to_csv(\"./labels.csv\", index=False)\n","\n","# image manipulation\n","labels = pd.read_csv(\"./labels.csv\")\n","labels = labels.values\n","image_path = glob.glob(str('./data/original') + str(\"/*\"))\n","for i in range(len(image_path)):\n","    single_mask = labels[i][1]\n","    single_img = image_path[i]\n","    img = Image.open(single_img)\n","    img1 = cv2.imread(single_img)\n","    result = feature_points(np.array(img))\n","    result1 = feature_points(img1)\n","    keypoints = []\n","    for item in result:\n","        k = cv2.KeyPoint(item[1],item[0],item[2])\n","        keypoints.append(k)\n","    keypoints1 = []\n","    for item in result1:\n","        k = cv2.KeyPoint(item[1], item[0], item[2])\n","        keypoints1.append(k)\n","    x = cv2.drawKeypoints(img1, np.array(keypoints), None)\n","    x1 = cv2.drawKeypoints(img1, np.array(keypoints1), None)\n","    location = \"./data/change1/{}.jpg\".format(labels[i][0])\n","    loation1 = \"./data/change2/{}.jpg\".format(labels[i][0])\n","    cv2.imwrite(location, x)\n","    cv2.imwrite(loation1, x1)\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iKwaWTKO9raT","colab_type":"code","colab":{}},"source":["import torch\n","from torch import nn, optim\n","import torch.nn.functional as F\n","import numpy as np\n","import cv2 as cv\n","import os\n","import time\n","import matplotlib\n","import matplotlib.pyplot as plt\n","\n","\n","def CrossEntropyLoss(outputs, targets, alpha=1, gamma=5):\n","  cross = -(targets*torch.log(outputs) + (1 - targets)*torch.log(1-outputs))\n","  return torch.mean(cross)\n","\n","\n","def FocalLoss(outputs, targets, alpha=1, gamma=5):\n","  cross = -(targets*torch.log(outputs) + (1 - targets)*torch.log(1-outputs))\n","  pt = torch.exp(cross)\n","  loss = (alpha*(1 - pt)**gamma)*cross\n","  return torch.mean(loss)\n","\n","def weights_init(m):\n","\n","    if type(m) == nn.Linear:\n","        y = m.in_features\n","        m.weight.data.normal_(0.0,1/np.sqrt(y))\n","        m.bias.data.fill_(0)\n","    elif type(m) == nn.Conv2d:\n","      m.weight.data.normal_(0.0,2/np.sqrt(m.in_channels))\n","      m.bias.data.fill_(0)\n","\n","class ConvCoords(nn.Module):\n","\n","    def __init__(self, in_channels, out_channels=64, kernel=7, padding=4, stride=2):\n","        super().__init__()\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(in_channels + 2, out_channels, kernel_size=kernel, padding=padding, stride=stride),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(),)\n","\n","\n","    def forward(self, input_):\n","\n","        b, _, w, h = input_.size()\n","\n","        x_coords = torch.arange(w).repeat(1, h, 1)\n","        y_coords = torch.arange(h).repeat(1, w, 1).transpose(1, 2)\n","\n","        x_coords = x_coords.float() / (w - 1)\n","        y_coords = y_coords.float() / (h - 1)\n","\n","        #centering coordinates around image centre\n","        x_coords = 2*x_coords - 1\n","        y_coords = 2*y_coords - 1\n","\n","        x_coords = x_coords.repeat(b, 1, 1, 1).transpose(2, 3)\n","        y_coords = y_coords.repeat(b, 1, 1, 1).transpose(2, 3)\n","\n","        coord = torch.cat((input_, x_coords.type_as(input_), y_coords.type_as(input_)), dim=1)\n","\n","        return self.conv(coord)\n","\n","class ResBlock(nn.Module):\n","  def __init__(self,num_in_channels, out_channels, kernel=3, stride=1):\n","        super(ResBlock, self).__init__()\n","\n","        padding = kernel // 2\n","        self.stride = stride\n","        self.conv1 = nn.Sequential(\n","            nn.Conv2d(num_in_channels, out_channels, kernel_size=kernel, padding=padding,stride=stride),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(),)\n","        \n","        self.conv2 = nn.Sequential(\n","            nn.Conv2d(out_channels, out_channels, kernel_size=kernel, padding=padding),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(),)\n","        \n","        if (stride == 2):\n","          self.conv_reshape = nn.Conv2d(num_in_channels, out_channels, kernel_size=1,stride=2)\n","        \n","  def forward(self, x):\n","    out = self.conv1(x)\n","    out = self.conv2(out)\n","    if self.stride == 1:\n","      out += x\n","    elif self.stride == 2:\n","      out += self.conv_reshape(x)\n","    return out\n","\n","\n","class Percep(nn.Module):\n","  def __init__(self):\n","    super(Percep, self).__init__()\n","\n","    self.linear_reg = nn.Sequential(nn.Linear(128*128*3, 512),\n","                    nn.ReLU(),\n","                    nn.Linear(512, 1),\n","                    nn.Sigmoid())\n","    \n","  def forward(self, x):\n","    b = x.size(0)\n","    out = x.view(b,-1)\n","    return self.linear_reg(out)\n","\n","\n","class ResNet(nn.Module):\n","  def __init__(self, in_channels, num_filters=64, kernel=3):\n","    super(ResNet, self).__init__()\n","    self.conv_coords = nn.Sequential(\n","        ConvCoords(in_channels,out_channels=num_filters),\n","        nn.MaxPool2d(3,stride=2))\n","    self.res1 = ResBlock(num_filters,num_filters)\n","\n","    self.res2 = nn.Sequential(\n","        ResBlock(num_filters,num_filters*2, stride=2),\n","        ResBlock(num_filters*2,num_filters*2))\n","\n","    self.res3 = nn.Sequential(\n","        ResBlock(num_filters*2,num_filters*4, stride=2),\n","        ResBlock(num_filters*4,num_filters*4))\n","     \n","    self.res4 = nn.Sequential(\n","        ResBlock(num_filters*4,num_filters*8, stride=2),\n","        ResBlock(num_filters*8,num_filters*8))\n","    \n","    self.res5 = nn.Sequential(\n","        ResBlock(num_filters*8,num_filters*16, stride=2),\n","        ResBlock(num_filters*16,num_filters*16))\n","    \n","    self.linear_reg = nn.Sequential(nn.Linear(num_filters*64, 1),\n","                    nn.Sigmoid())\n","    \n","  def forward(self, x):\n","    b = x.size(0)\n","    out = self.conv_coords(x)\n","    out = self.res1(out)\n","    out = self.res2(out)\n","    out = self.res3(out)\n","    out = self.res4(out)\n","    out = self.res5(out)\n","    out = out.view(b,-1)\n","    return 0.99*self.linear_reg(out)+0.001\n","\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UrxfRIiU3ORJ","colab_type":"code","colab":{}},"source":["def train(type, dataset, in_channels=3, epochs=10, lr=0.001, wd=1, clip=None):\n","    if (type == 'res'):\n","      net = ResNet(in_channels)\n","    else:\n","      net = Percep()\n","    \n","    net.apply(weights_init)\n","\n","    torch.cuda.empty_cache()\n","    net.float()\n","    net.cuda()\n","    optimizer = torch.optim.Adam(net.parameters(), lr=lr,weight_decay=wd)\n","\n","    # DATA\n","    print(\"Loading data...\")\n","    # Normal Charts\n","    if (dataset == 0):\n","      path = \"/content/drive/My Drive/CSC420P/data/original/\"\n","    # Augmeted with SIFT\n","    elif (dataset == 1):\n","      path = \"/content/drive/My Drive/CSC420P/data/change1/\"\n","    # RC Plot\n","    else:\n","      path = \"/content/drive/My Drive/CSC420P/data/RCplot1/\"\n","\n","    trainLoad, validLoad, testLoad = prep_data(path, \"/content/drive/My Drive/CSC420P/labels.csv\")\n","\n","    start = time.time()\n","\n","    train_losses = []\n","    valid_losses = []\n","    valid_accs = []\n","    for epoch in range(epochs):\n","        # Train the Model\n","        net.train() # Change model to 'train' mode\n","        losses = []\n","        for batch, (images, masks) in enumerate(trainLoad):\n","          # Forward + Backward + Optimize\n","          optimizer.zero_grad()\n","          outputs = net(images)\n","          \n","          loss = CrossEntropyLoss(outputs,masks)\n","          loss.backward()\n","          if clip is not None:\n","            nn.utils.clip_grad_value_(net.parameters(),clip)\n","          \n","          torch.cuda.empty_cache()\n","          optimizer.step()\n","          losses.append(loss.data.item())\n","          del loss\n","          torch.cuda.empty_cache()\n","        \n","        avg_loss = np.mean(losses)\n","        del losses\n","        torch.cuda.empty_cache()\n","        train_losses.append(avg_loss)\n","        time_elapsed = time.time() - start\n","        print('Epoch [%d/%d], Loss: %.4f, Time (s): %d' % (\n","            epoch+1, epochs, avg_loss, time_elapsed))\n","\n","        # Evaluate the model\n","        net.eval()  # Change model to 'eval' mode (BN uses moving mean/var).\n","        val_loss = 0\n","        for batch, (images, masks) in enumerate(validLoad):\n","          output = net(images)\n","          val_loss += CrossEntropyLoss(output, masks)\n","          del output\n","        val_loss = val_loss/len(validLoad)\n","        time_elapsed = time.time() - start\n","        valid_losses.append(val_loss.data.item())\n","        print('Epoch [%d/%d], Val Loss: %.4f, Time(s): %d' % (\n","            epoch+1, epochs, val_loss, time_elapsed))\n","        torch.cuda.empty_cache()\n","    test_loss = 0\n","    for batch, (images, masks) in enumerate(testLoad):\n","        output = net(images)\n","        test_loss += CrossEntropyLoss(output, masks)\n","        del output\n","\n","    test_loss = test_loss/len(testLoad) \n","    print(\"Test Loss: \", test_loss)\n","    del test_loss\n","    # Plot training curve\n","    plt.figure()\n","    plt.plot(train_losses, \"ro-\", label=\"Train\")\n","    plt.plot(valid_losses, \"go-\", label=\"Validation\")\n","    plt.legend()\n","    plt.title(\"Loss\")\n","    plt.xlabel(\"Epochs\")\n","    plt.savefig(\"training_curve.png\")\n","\n","    \n","    print('Saving model...')\n","    torch.save(net.state_dict(), \"net.pth\")\n","    \n","    return net"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yMAbgnDjZZcJ","colab_type":"code","colab":{}},"source":["torch.manual_seed(693)\n","r = train(\"res\", 0, 3,11,clip=1,wd=100)\n","torch.manual_seed(693)\n","r1 = train(\"res\", 1, 3,11,clip=1,wd=100)\n","torch.manual_seed(693)\n","r2 = train(\"res\", 2, 3,11,clip=1,wd=100)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Fkfx5OXPa93","colab_type":"code","colab":{}},"source":["torch.manual_seed(693)\n","r = train(\"percep\", 0, epochs=11,lr=0.0001,clip=1)\n","torch.manual_seed(693)\n","r1 = train(\"percep\", 1, epochs=9,lr=0.0001,clip=1)\n","torch.manual_seed(693)\n","r2 = train(\"percep\", 2, epochs=9,lr=0.0001,clip=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"okay7gOKqLmY","colab_type":"code","colab":{}},"source":["def accuracy(net, dataset):\n","    if (dataset == 0):\n","      path = \"/content/drive/My Drive/CSC420P/data/original/\"\n","    # Augmeted with SIFT\n","    elif (dataset == 1):\n","      path = \"/content/drive/My Drive/CSC420P/data/change1/\"\n","    # RC Plot\n","    else:\n","      path = \"/content/drive/My Drive/CSC420P/data/RCplot1/\"\n","    trainLoad, validLoad, testLoad = prep_data(path, \"/content/drive/My Drive/CSC420P/labels.csv\")\n","    testacc = 0\n","    for batch, (img, mask) in enumerate(testLoad):\n","      out = net(img)\n","      for i in range(img.size(0)):\n","        if out[i,0] > 0.5 and mask[i] > 0.5:\n","          testacc += 1\n","        elif out[i,0] < 0.5 and mask[i] < 0.5:\n","          testacc+= 1\n","    testacc/75\n","    acc = 0\n","    for batch, (img, mask) in enumerate(validLoad):\n","      out = net(img)\n","      for i in range(img.size(0)):\n","        if out[i,0] > 0.5 and mask[i] > 0.5:\n","          acc += 1\n","        elif out[i,0] < 0.5 and mask[i] < 0.5:\n","          acc+= 1\n","    print(acc/75, testacc/75)"],"execution_count":0,"outputs":[]}]}
